{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**"
      ],
      "metadata": {
        "id": "LxWWvZL9aZel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing functions"
      ],
      "metadata": {
        "id": "6Wk6hJpYbi9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import *\n",
        "from google.colab import drive, files"
      ],
      "metadata": {
        "id": "kzNgSLKVbqCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1 - Reading files and extracting data"
      ],
      "metadata": {
        "id": "_Svk98-ZtwkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading file\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/dataset1.csv')\n",
        "\n",
        "# Reading dataset and separating input features (X) and target labels (y)\n",
        "X = data.iloc[:, :-1]  # Input features\n",
        "y = data.iloc[:, -1]   # Target labels (1 for hitting, 0 for missing)\n",
        "\n",
        "\n",
        "# Finding scaling parameters using mean and standard deviation and storing a numpy array\n",
        "mean = X.mean()\n",
        "std = X.std()\n",
        "\n",
        "# COnverting to a 2 x 6 numpy array\n",
        "scale_array = np.vstack((mean, std))\n",
        "\n",
        "# Save the features array using np.savetxt\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/at2720-1.txt', scale_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGmmvQG1bu0N",
        "outputId": "0dc6c586-a003-4619-b5ec-29059e61c479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining training sets and applying scaling to data"
      ],
      "metadata": {
        "id": "qLg2OYeYtvNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load scaling parameters from the saved text file\n",
        "scale_array = np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/at2720-1.txt')\n",
        "\n",
        "# Applying scaling to the dataset\n",
        "X_array = X.values\n",
        "\n",
        "print(scale_array)\n",
        "\n",
        "X_scaled = (X_array - scale_array[0])/scale_array[1]\n",
        "y_array = y.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX57QrrenQWa",
        "outputId": "dfd861ec-504f-48a7-97c8-2adb40daca4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.99790586e-01 3.10500234e-01 1.61670426e+02 1.88135000e+01\n",
            "  2.84024574e+02 4.27726820e+00]\n",
            " [9.83924728e-02 9.97600486e-02 8.83980565e+01 4.82435021e+00\n",
            "  1.37933777e+02 2.21237117e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining training and testing dataset"
      ],
      "metadata": {
        "id": "cNocFxI49lUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_array, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_binary = to_categorical(y_train)\n",
        "y_test_binary = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "8I8fGZiB9o6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to find the Optimal Neural Network to this dataset and the time taken"
      ],
      "metadata": {
        "id": "UQkoMuBh9LYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a function to create and compile the model\n",
        "# def create_model(units, activation, input_dim):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(units=units, activation='relu', input_dim=input_dim))\n",
        "#     model.add(Dense(units=units/2, activation=activation))\n",
        "#     model.add(Dense(units=2, activation='sigmoid'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "\n",
        "# # Define hyperparameters\n",
        "# units_list = [6,12] # nodes\n",
        "# activations = ['relu', 'softmax', 'tanh', 'softplus', 'sigmoid']\n",
        "\n",
        "# # Store results (accuracy, model)\n",
        "# results = []\n",
        "\n",
        "# # Iterate through hyperparameters\n",
        "# for units in units_list:\n",
        "#     for activation in activations:\n",
        "#         # Calculating time to implement the model\n",
        "#         st = time.time()\n",
        "#         # Create and compile the model\n",
        "#         model = create_model(units, activation, X_train.shape[1])\n",
        "\n",
        "#         # Train the model\n",
        "#         model.fit(X_train, y_train_binary, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "#         et = time.time()\n",
        "\n",
        "#         # Evaluate the model\n",
        "#         _, accuracy = model.evaluate(X_test, y_test_binary, verbose=0)\n",
        "\n",
        "#         # Store the results\n",
        "#         results.append((units, activation, accuracy, model))\n",
        "\n",
        "#         # Print the accuracy for each configuration\n",
        "#         print(f\"Units: {units}, Activation: {activation}, Accuracy: {accuracy}, Time: {et-st} ms\")\n",
        "\n",
        "# # Find the best configuration and accuracy\n",
        "# best_config = max(results, key=lambda x: x[2])\n",
        "# best_units, best_activation, best_accuracy, best_model = best_config\n",
        "\n",
        "# # Print the best configuration and accuracy\n",
        "# print(f\"Best Configuration: Units={best_units}, Activation={best_activation}, Accuracy={best_accuracy}\")"
      ],
      "metadata": {
        "id": "IxgoE6Xg9P05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the most optimal solution for dataset_1"
      ],
      "metadata": {
        "id": "MpohBlxnUlbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=12, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=12, activation='relu'))\n",
        "model.add(Dense(units=2, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train_binary, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "# Save the model in HDF5 format\n",
        "model.save('/content/gdrive//MyDrive/Colab Notebooks/at2720-1.h5')"
      ],
      "metadata": {
        "id": "Q7OLO6PvUkUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664eb48c-d6d4-4270-bd8a-c84155755fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2 - Reading files and extracting data"
      ],
      "metadata": {
        "id": "HgWOtVCmReQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading file\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/dataset2.csv')\n",
        "\n",
        "# Reading dataset and separating input features (X) and target labels (y)\n",
        "X = data.iloc[:, :-1]  # Input features\n",
        "y = data.iloc[:, -1]   # Target labels (1 for hitting, 0 for missing)\n",
        "\n",
        "\n",
        "# Finding scaling parameters using mean and standard deviation and storing a numpy array\n",
        "mean = X.mean()\n",
        "std = X.std()\n",
        "\n",
        "# COnverting to a 2 x 6 numpy array\n",
        "scale_array = np.vstack((mean, std))\n",
        "\n",
        "# Save the features array using np.savetxt\n",
        "np.savetxt('/content/gdrive/MyDrive/Colab Notebooks/at2720-2.txt', scale_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLig3ufCRkxw",
        "outputId": "09b49dd6-bb03-423d-8a22-07912bd8253f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining training sets and applying scaling to data"
      ],
      "metadata": {
        "id": "n090SsdhRqAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load scaling parameters from the saved text file\n",
        "scale_array = np.loadtxt('/content/gdrive/MyDrive/Colab Notebooks/at2720-2.txt')\n",
        "\n",
        "# Applying scaling to the dataset\n",
        "X_array = X.values\n",
        "\n",
        "print(scale_array)\n",
        "\n",
        "X_scaled = (X_array - scale_array[0])/scale_array[1]\n",
        "y_array = y.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9D-hN5tRw34",
        "outputId": "75a847ee-4cd6-4729-ac00-0032f9946764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.02567021e-01 3.11477025e-01 5.19591181e+01 1.89642500e+01\n",
            "  6.19341255e+02 4.20296119e+00]\n",
            " [9.92623853e-02 9.82182147e-02 2.93926520e+01 4.87415750e+00\n",
            "  3.69852469e+02 2.16036380e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining training and testing dataset"
      ],
      "metadata": {
        "id": "uoJRgz73R2RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_array, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_binary = to_categorical(y_train)\n",
        "y_test_binary = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "utDGbFoaR5if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code to find Optimal Neural Network to this dataset and the time taken"
      ],
      "metadata": {
        "id": "mCfFGeGPbqNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create and compile the model\n",
        "def create_model(units, activation, input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=units, activation='tanh', input_dim=input_dim))\n",
        "    model.add(Dense(units=12, activation='relu'))\n",
        "    model.add(Dense(units=6, activation='tanh'))\n",
        "    model.add(Dense(units=4, activation='tanh'))\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "units = 4 # nodes\n",
        "\n",
        "# Store results (accuracy, model)\n",
        "results = []\n",
        "\n",
        "\n",
        "# Calculating time to implement the model\n",
        "st = time.time()\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model(units, activation, X_train.shape[1])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train_binary, epochs=200, batch_size=16, verbose=0)\n",
        "\n",
        "et = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(X_test, y_test_binary, verbose=0)\n",
        "\n",
        "# Store the results\n",
        "results.append((units, activation, accuracy, model))\n",
        "\n",
        "# Print the accuracy for each configuration\n",
        "print(f\"Units: {units}, Activation: {activation}, Accuracy: {accuracy}, Time: {et-st}\")\n",
        "\n",
        "# Find the best configuration and accuracy\n",
        "best_config = max(results, key=lambda x: x[2])\n",
        "best_units, best_activation, best_accuracy, best_model = best_config\n",
        "\n",
        "# Print the best configuration and accuracy\n",
        "print(f\"Best Configuration: Units={best_units}, Activation={best_activation}, Accuracy={best_accuracy}\")\n",
        "\n",
        "# Save the model in HDF5 format\n",
        "model.save('/content/gdrive//MyDrive/Colab Notebooks/at2720-2.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2S0cJXCb2Ak",
        "outputId": "a98c56cb-a4d3-429a-d95b-7d496d5a9cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Units: 4, Activation: tanh, Accuracy: 0.8212500214576721, Time: 99.15528774261475\n",
            "Best Configuration: Units=4, Activation=tanh, Accuracy=0.8212500214576721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zff-4X-kn1vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the most optimal solution for dataset_2"
      ],
      "metadata": {
        "id": "af917wgpb2t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 79.3 = 4,8,12,6,2 - tanh relu and ends with softmax\n",
        "\n",
        "#79.25 - 4,6,12,6,2 tanh relu relu tanh softmax 16 batch size\n",
        "#79.75 = 4,6,12,6,2 tanh relu relu tanh softmax\n",
        "\n",
        "\n",
        "#78.9 = 4,6,12,6,2 tanh, relu, tanh, relu softmax bs = 16\n",
        "#79 = 4,8,12,6,2 tanh, relu, tanh, relu softmax bs = 16\n",
        "\n",
        "# 4, 6 , 6 , 4 , 2\n",
        "\n"
      ],
      "metadata": {
        "id": "g3i5B0wkb6uj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}